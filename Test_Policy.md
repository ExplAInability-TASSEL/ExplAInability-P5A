# **EXPLAINABILITY test policy**:

The purpose of this document is to outline the testing approach for the EXPLAINABILITY project. This test policy aims to ensure the quality and reliability of the project deliverables.

We acknowledge that automated testing may not be necessary for the scope of this project. However, we are committed to conducting comprehensive functional and performance testing to validate the functionality and efficiency of the EXPLAINABILITY application.

<u>**Test Types:**</u>

a. **Functional Testing:**
* Verify that data preprocessing is accurately executed.
* Ensure the correctness of the classification process.
* Confirm the seamless integration of data into the interactive web map.
* Validate the overall functionality of the TASSEL application.

b. **Performance Testing:**
* Assess the system's responsiveness under varying data loads.
* Evaluate the application's performance in terms of data processing and map rendering.
* Identify and address any performance bottlenecks.

<u>**Test Environment:**</u>

The testing will be conducted in an environment that closely mirrors the production setup. This includes utilising representative datasets and simulating user interactions to assess the system's behavior under real-world conditions.

<u>**Test Documentation:**</u>

All test cases, test plans, and test results will be thoroughly documented to provide transparency and facilitate future maintenance.

<u>**Test Schedule:**</u>

Testing activities will be integrated into the project development lifecycle.

<u>**Exceptions to Automated Testing**</u>

It is explicitly documented that automated testing will not be pursued for TASSEL. The decision is based on the assessment that the project's complexity and requirements do not necessitate the overhead of automated testing.